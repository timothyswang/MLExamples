{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Wang_Timothy_HW3</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Timothy Wang\n",
    "<br>\n",
    "Github Username: timothyswang\n",
    "<br>\n",
    "USC ID: 2697040790"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import bootstrap\n",
    "# import bootstrapped.bootstrap as boot\n",
    "# import bootstrapped.stats_functions as bootstrap_stat\n",
    "\n",
    "# bootstrapped.bootstrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_bending1 = \"../data/AReM/bending1\"\n",
    "directory_bending2 = \"../data/AReM/bending2\"\n",
    "directory_cycling = \"../data/AReM/cycling\"\n",
    "directory_lying = \"../data/AReM/lying\"\n",
    "directory_sitting = \"../data/AReM/sitting\"\n",
    "directory_standing = \"../data/AReM/standing\"\n",
    "directory_walking = \"../data/AReM/walking\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = []\n",
    "data_train = []\n",
    "data_all = []\n",
    "\n",
    "data_test_names = []\n",
    "data_train_names = []\n",
    "\n",
    "#[1]\n",
    "for this_file in os.listdir(directory_bending1):\n",
    "    path_this_file = directory_bending1 + \"/\" + this_file\n",
    "    #[2]\n",
    "    df_this_file = pd.read_csv(path_this_file)\n",
    "\n",
    "    if this_file == \"dataset1.csv\" or this_file == \"dataset2.csv\":\n",
    "        # print(this_file)\n",
    "        data_test.append(df_this_file)\n",
    "        data_test_names.append(path_this_file)\n",
    "    else:\n",
    "        data_train.append(df_this_file)\n",
    "        # print(this_file)\n",
    "        data_train_names.append(path_this_file)\n",
    "\n",
    "    data_all.append(df_this_file)\n",
    "\n",
    "#[1]\n",
    "for this_file in os.listdir(directory_bending2):\n",
    "    path_this_file = directory_bending2 + \"/\" + this_file\n",
    "\n",
    "    #[2]\n",
    "    df_this_file = pd.read_csv(path_this_file)\n",
    "\n",
    "    if this_file == \"dataset1.csv\" or this_file == \"dataset2.csv\":\n",
    "        # print(this_file)\n",
    "        data_test.append(df_this_file)\n",
    "        data_test_names.append(path_this_file)\n",
    "    else:\n",
    "        data_train.append(df_this_file)\n",
    "        # print(this_file)\n",
    "        data_train_names.append(path_this_file)\n",
    "\n",
    "    data_all.append(df_this_file)\n",
    "\n",
    "#[1]\n",
    "for this_file in os.listdir(directory_cycling):\n",
    "    path_this_file = directory_cycling + \"/\" + this_file\n",
    "\n",
    "    #[2]\n",
    "    df_this_file = pd.read_csv(path_this_file)\n",
    "\n",
    "    if this_file == \"dataset1.csv\" or this_file == \"dataset2.csv\" or this_file == \"dataset3.csv\":\n",
    "        # print(this_file)\n",
    "        data_test.append(df_this_file)\n",
    "        data_test_names.append(path_this_file)\n",
    "    else:\n",
    "        data_train.append(df_this_file)\n",
    "        # print(this_file)\n",
    "        data_train_names.append(path_this_file)\n",
    "\n",
    "    data_all.append(df_this_file)\n",
    "\n",
    "#[1]\n",
    "for this_file in os.listdir(directory_lying):\n",
    "    path_this_file = directory_lying + \"/\" + this_file\n",
    "\n",
    "    #[2]\n",
    "    df_this_file = pd.read_csv(path_this_file)\n",
    "\n",
    "    if this_file == \"dataset1.csv\" or this_file == \"dataset2.csv\" or this_file == \"dataset3.csv\":\n",
    "        # print(this_file)\n",
    "        data_test.append(df_this_file)\n",
    "        data_test_names.append(path_this_file)\n",
    "    else:\n",
    "        data_train.append(df_this_file)\n",
    "        # print(this_file)\n",
    "        data_train_names.append(path_this_file)\n",
    "\n",
    "    data_all.append(df_this_file)\n",
    "\n",
    "#[1]\n",
    "for this_file in os.listdir(directory_sitting):\n",
    "    path_this_file = directory_sitting + \"/\" + this_file\n",
    "\n",
    "    #[2]\n",
    "    df_this_file = pd.read_csv(path_this_file)\n",
    "\n",
    "    if this_file == \"dataset1.csv\" or this_file == \"dataset2.csv\" or this_file == \"dataset3.csv\":\n",
    "        # print(this_file)\n",
    "        data_test.append(df_this_file)\n",
    "        data_test_names.append(path_this_file)\n",
    "    else:\n",
    "        data_train.append(df_this_file)\n",
    "        # print(this_file)\n",
    "        data_train_names.append(path_this_file)\n",
    "\n",
    "    data_all.append(df_this_file)\n",
    "\n",
    "#[1]\n",
    "for this_file in os.listdir(directory_standing):\n",
    "    path_this_file = directory_standing + \"/\" + this_file\n",
    "\n",
    "    #[2]\n",
    "    df_this_file = pd.read_csv(path_this_file)\n",
    "\n",
    "    if this_file == \"dataset1.csv\" or this_file == \"dataset2.csv\" or this_file == \"dataset3.csv\":\n",
    "        # print(this_file)\n",
    "        data_test.append(df_this_file)\n",
    "        data_test_names.append(path_this_file)\n",
    "    else:\n",
    "        data_train.append(df_this_file)\n",
    "        # print(this_file)\n",
    "        data_train_names.append(path_this_file)\n",
    "\n",
    "    data_all.append(df_this_file)\n",
    "\n",
    "#[1]\n",
    "for this_file in os.listdir(directory_walking):\n",
    "    path_this_file = directory_walking + \"/\" + this_file\n",
    "\n",
    "    #[2]\n",
    "    df_this_file = pd.read_csv(path_this_file)\n",
    "\n",
    "    if this_file == \"dataset1.csv\" or this_file == \"dataset2.csv\" or this_file == \"dataset3.csv\":\n",
    "        # print(this_file)\n",
    "        data_test.append(df_this_file)\n",
    "        data_test_names.append(path_this_file)\n",
    "    else:\n",
    "        data_train.append(df_this_file)\n",
    "        # print(this_file)\n",
    "        data_train_names.append(path_this_file)\n",
    "\n",
    "    data_all.append(df_this_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Research"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some forms of \"primitive\" time-domain features in time series classification includes mean, standard deviation, and slope [3] (as well as the other features that we'll look at in the next part, such as minimum, maximum, median, first quartile, and third quartile). Other features could be other series, such as a new series created using Fourier transforms, or fitted auto-regressive coefficients [3].\n",
    "\n",
    "Other series type features can include a difference transform (where you take the difference in values between each consecutive datapoints) -- this is known as \"first order difference\" [4]. Another series type featurer is standardization, where you subtract the mean and divide by the standard deviation such that you get a Gaussian (normal) distribution [4]. You could have a power transform, such as taking the log of all of your values (only if your values are positive) [4]. Finally, you could use normalization, where you rescale your data range to be between 0 and 1 [4]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain_features = []\n",
    "instance = 1\n",
    "\n",
    "# testing_df = data_test[0]\n",
    "# print(testing_df.describe())\n",
    "\n",
    "def get_time_features(this_df, test_or_train, instance):\n",
    "    avg_rss12_min = this_df.describe()[\"avg_rss12\"][\"min\"]\n",
    "    avg_rss12_max = this_df.describe()[\"avg_rss12\"][\"max\"]\n",
    "    avg_rss12_mean = this_df.describe()[\"avg_rss12\"][\"mean\"]\n",
    "    avg_rss12_median = this_df.describe()[\"avg_rss12\"][\"50%\"]\n",
    "    avg_rss12_std = this_df.describe()[\"avg_rss12\"][\"std\"]\n",
    "    avg_rss12_firstq = this_df.describe()[\"avg_rss12\"][\"25%\"]\n",
    "    avg_rss12_thirdq = this_df.describe()[\"avg_rss12\"][\"75%\"]\n",
    "    avg_rss12_all = [avg_rss12_min, avg_rss12_max, avg_rss12_mean, avg_rss12_median, avg_rss12_std, avg_rss12_firstq, avg_rss12_thirdq]\n",
    "\n",
    "    var_rss12_min = this_df.describe()[\"var_rss12\"][\"min\"]\n",
    "    var_rss12_max = this_df.describe()[\"var_rss12\"][\"max\"]\n",
    "    var_rss12_mean = this_df.describe()[\"var_rss12\"][\"mean\"]\n",
    "    var_rss12_median = this_df.describe()[\"var_rss12\"][\"50%\"]\n",
    "    var_rss12_std = this_df.describe()[\"var_rss12\"][\"std\"]\n",
    "    var_rss12_firstq = this_df.describe()[\"var_rss12\"][\"25%\"]\n",
    "    var_rss12_thirdq = this_df.describe()[\"var_rss12\"][\"75%\"]\n",
    "    var_rss12_all = [var_rss12_min, var_rss12_max, var_rss12_mean, var_rss12_median, var_rss12_std, var_rss12_firstq, var_rss12_thirdq]\n",
    "\n",
    "    avg_rss13_max = this_df.describe()[\"avg_rss13\"][\"max\"]\n",
    "    avg_rss13_min = this_df.describe()[\"avg_rss13\"][\"min\"]\n",
    "    avg_rss13_mean = this_df.describe()[\"avg_rss13\"][\"mean\"]\n",
    "    avg_rss13_median = this_df.describe()[\"avg_rss13\"][\"50%\"]\n",
    "    avg_rss13_std = this_df.describe()[\"avg_rss13\"][\"std\"]\n",
    "    avg_rss13_firstq = this_df.describe()[\"avg_rss13\"][\"25%\"]\n",
    "    avg_rss13_thirdq = this_df.describe()[\"avg_rss13\"][\"75%\"]\n",
    "    avg_rss13_all = [avg_rss13_min, avg_rss13_max, avg_rss13_mean, avg_rss13_median, avg_rss13_std, avg_rss13_firstq, avg_rss13_thirdq]\n",
    "\n",
    "    var_rss13_min = this_df.describe()[\"var_rss13\"][\"min\"]\n",
    "    var_rss13_max = this_df.describe()[\"var_rss13\"][\"max\"]\n",
    "    var_rss13_mean = this_df.describe()[\"var_rss13\"][\"mean\"]\n",
    "    var_rss13_median = this_df.describe()[\"var_rss13\"][\"50%\"]\n",
    "    var_rss13_std = this_df.describe()[\"var_rss13\"][\"std\"]\n",
    "    var_rss13_firstq = this_df.describe()[\"var_rss13\"][\"25%\"]\n",
    "    var_rss13_thirdq = this_df.describe()[\"var_rss13\"][\"75%\"]\n",
    "    var_rss13_all = [var_rss13_min, var_rss13_max, var_rss13_mean, var_rss13_median, var_rss13_std, var_rss13_firstq, var_rss13_thirdq]\n",
    "\n",
    "    avg_rss23_max = this_df.describe()[\"avg_rss23\"][\"max\"]\n",
    "    avg_rss23_min = this_df.describe()[\"avg_rss23\"][\"min\"]\n",
    "    avg_rss23_mean = this_df.describe()[\"avg_rss23\"][\"mean\"]\n",
    "    avg_rss23_median = this_df.describe()[\"avg_rss23\"][\"50%\"]\n",
    "    avg_rss23_std = this_df.describe()[\"avg_rss23\"][\"std\"]\n",
    "    avg_rss23_firstq = this_df.describe()[\"avg_rss23\"][\"25%\"]\n",
    "    avg_rss23_thirdq = this_df.describe()[\"avg_rss23\"][\"75%\"]\n",
    "    avg_rss23_all = [avg_rss23_min, avg_rss23_max, avg_rss23_mean, avg_rss23_median, avg_rss23_std, avg_rss23_firstq, avg_rss23_thirdq]\n",
    "\n",
    "    var_rss23_min = this_df.describe()[\"var_rss23\"][\"min\"]\n",
    "    var_rss23_max = this_df.describe()[\"var_rss23\"][\"max\"]\n",
    "    var_rss23_mean = this_df.describe()[\"var_rss23\"][\"mean\"]\n",
    "    var_rss23_median = this_df.describe()[\"var_rss23\"][\"50%\"]\n",
    "    var_rss23_std = this_df.describe()[\"var_rss23\"][\"std\"]\n",
    "    var_rss23_firstq = this_df.describe()[\"var_rss23\"][\"25%\"]\n",
    "    var_rss23_thirdq = this_df.describe()[\"var_rss23\"][\"75%\"]\n",
    "    var_rss23_all = [var_rss23_min, var_rss23_max, var_rss23_mean, var_rss23_median, var_rss23_std, var_rss23_firstq, var_rss23_thirdq]\n",
    "\n",
    "    return [instance, test_or_train] + avg_rss12_all + var_rss12_all + avg_rss13_all + var_rss13_all + avg_rss23_all + var_rss23_all\n",
    "\n",
    "for this_df in data_test:\n",
    "    test_or_train = 0\n",
    "\n",
    "    this_instance_features = get_time_features(this_df=this_df,test_or_train=test_or_train, instance=instance)\n",
    "\n",
    "    time_domain_features.append(this_instance_features)\n",
    "\n",
    "    instance += 1\n",
    "\n",
    "for this_df in data_train:\n",
    "\n",
    "    test_or_train = 1\n",
    "\n",
    "    this_instance_features = get_time_features(this_df=this_df,test_or_train=test_or_train, instance=instance)\n",
    "\n",
    "    time_domain_features.append(this_instance_features)\n",
    "\n",
    "    instance += 1\n",
    "\n",
    "column_names_avg12 = [\"instance\", \"test_or_train\", \"avg12_min\", \"avg12_max\", \"avg12_mean\", \"avg12_median\", \"avg12_std\", \"avg12_firstq\", \"avg12_thirdq\"]\n",
    "column_names_var12 = [\"var12_min\", \"var12_max\", \"var12_mean\", \"var12_median\", \"var12_std\", \"var12_firstq\", \"var12_thirdq\"]\n",
    "column_names_avg13 = [\"avg13_min\", \"avg13_max\", \"avg13_mean\", \"avg13_median\", \"avg13_std\", \"avg13_firstq\", \"avg13_thirdq\"]\n",
    "column_names_var13 = [\"var13_min\", \"var13_max\", \"var13_mean\", \"var13_median\", \"var13_std\", \"var13_firstq\", \"var13_thirdq\"]\n",
    "column_names_avg23 = [\"avg23_min\", \"avg23_max\", \"avg23_mean\", \"avg23_median\", \"avg23_std\", \"avg23_firstq\", \"avg23_thirdq\"]\n",
    "column_names_var23 = [\"var23_min\", \"var23_max\", \"var23_mean\", \"var23_median\", \"var23_std\", \"var23_firstq\", \"var23_thirdq\"]\n",
    "\n",
    "column_names = column_names_avg12 + column_names_var12 + column_names_avg13 + column_names_var13 + column_names_avg23 + column_names_var23\n",
    "\n",
    "time_domain_df = pd.DataFrame(np.array(time_domain_features), columns=column_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance         25.547342\n",
      "test_or_train     0.413809\n",
      "avg12_min         9.569975\n",
      "avg12_max         4.394362\n",
      "avg12_mean        5.335718\n",
      "avg12_median      5.440054\n",
      "avg12_std         1.772153\n",
      "avg12_firstq      6.153590\n",
      "avg12_thirdq      5.138925\n",
      "var12_min         0.000000\n",
      "var12_max         5.062729\n",
      "var12_mean        1.574164\n",
      "var12_median      1.412244\n",
      "var12_std         0.884105\n",
      "var12_firstq      0.946386\n",
      "var12_thirdq      2.125266\n",
      "avg13_min         2.956462\n",
      "avg13_max         4.875137\n",
      "avg13_mean        4.008380\n",
      "avg13_median      4.036396\n",
      "avg13_std         0.946710\n",
      "avg13_firstq      4.220658\n",
      "avg13_thirdq      4.171628\n",
      "var13_min         0.000000\n",
      "var13_max         2.183625\n",
      "var13_mean        1.166114\n",
      "var13_median      1.145586\n",
      "var13_std         0.458242\n",
      "var13_firstq      0.843620\n",
      "var13_thirdq      1.552504\n",
      "avg23_min         6.124001\n",
      "avg23_max         5.741238\n",
      "avg23_mean        5.675593\n",
      "avg23_median      5.813782\n",
      "avg23_std         1.024898\n",
      "avg23_firstq      6.096465\n",
      "avg23_thirdq      5.531720\n",
      "var23_min         0.045838\n",
      "var23_max         2.518921\n",
      "var23_mean        1.154812\n",
      "var23_median      1.086474\n",
      "var23_std         0.517617\n",
      "var23_firstq      0.758584\n",
      "var23_thirdq      1.523599\n",
      "Name: std, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print(time_domain_df.describe())\n",
    "time_domain_stds_df = time_domain_df.describe().iloc[2]\n",
    "print(time_domain_stds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviations of the time-domain features:\n",
    "\n",
    "# avg12_min         9.569975\n",
    "# avg12_max         4.394362\n",
    "# avg12_mean        5.335718\n",
    "# avg12_median      5.440054\n",
    "# avg12_std         1.772153\n",
    "# avg12_firstq      6.153590\n",
    "# avg12_thirdq      5.138925\n",
    "# var12_min         0.000000\n",
    "# var12_max         5.062729\n",
    "# var12_mean        1.574164\n",
    "# var12_median      1.412244\n",
    "# var12_std         0.884105\n",
    "# var12_firstq      0.946386\n",
    "# var12_thirdq      2.125266\n",
    "# avg13_min         2.956462\n",
    "# avg13_max         4.875137\n",
    "# avg13_mean        4.008380\n",
    "# avg13_median      4.036396\n",
    "# avg13_std         0.946710\n",
    "# avg13_firstq      4.220658\n",
    "# avg13_thirdq      4.171628\n",
    "# var13_min         0.000000\n",
    "# var13_max         2.183625\n",
    "# var13_mean        1.166114\n",
    "# var13_median      1.145586\n",
    "# var13_std         0.458242\n",
    "# var13_firstq      0.843620\n",
    "# var13_thirdq      1.552504\n",
    "# avg23_min         6.124001\n",
    "# avg23_max         5.741238\n",
    "# avg23_mean        5.675593\n",
    "# avg23_median      5.813782\n",
    "# avg23_std         1.024898\n",
    "# avg23_firstq      6.096465\n",
    "# avg23_thirdq      5.531720\n",
    "# var23_min         0.045838\n",
    "# var23_max         2.518921\n",
    "# var23_mean        1.154812\n",
    "# var23_median      1.086474\n",
    "# var23_std         0.517617\n",
    "# var23_firstq      0.758584\n",
    "# var23_thirdq      1.523599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time_series_feature  lower_bound  upper_bound\n",
      "0             instance    23.184488    27.217100\n",
      "1        test_or_train     0.354829     0.450980\n",
      "2            avg12_min     8.212807    10.750130\n",
      "3            avg12_max     3.330210     5.238142\n",
      "4           avg12_mean     4.681892     5.838307\n",
      "5         avg12_median     4.764000     5.973226\n",
      "6            avg12_std     1.558504     1.939846\n",
      "7         avg12_firstq     5.534561     6.605173\n",
      "8         avg12_thirdq     4.302042     5.814322\n",
      "9            var12_min     0.000000     0.000000\n",
      "10           var12_max     4.593852     5.378349\n",
      "11          var12_mean     1.384899     1.696430\n",
      "12        var12_median     1.228041     1.535231\n",
      "13           var12_std     0.798626     0.936528\n",
      "14        var12_firstq     0.827668     1.027429\n",
      "15        var12_thirdq     1.888248     2.281940\n",
      "16           avg13_min     2.742078     3.089455\n",
      "17           avg13_max     4.149385     5.429403\n",
      "18          avg13_mean     3.393033     4.468042\n",
      "19        avg13_median     3.411116     4.498944\n",
      "20           avg13_std     0.760545     1.117162\n",
      "21        avg13_firstq     3.629342     4.684571\n",
      "22        avg13_thirdq     3.520459     4.658584\n",
      "23           var13_min     0.000000     0.000000\n",
      "24           var13_max     1.959137     2.346481\n",
      "25          var13_mean     1.069181     1.215736\n",
      "26        var13_median     1.049430     1.194355\n",
      "27           var13_std     0.419213     0.483073\n",
      "28        var13_firstq     0.770143     0.884846\n",
      "29        var13_thirdq     1.424464     1.617284\n",
      "30           avg23_min     4.379803     7.440026\n",
      "31           avg23_max     4.730979     6.528197\n",
      "32          avg23_mean     4.397777     6.676646\n",
      "33        avg23_median     4.486465     6.877413\n",
      "34           avg23_std     0.809832     1.216205\n",
      "35        avg23_firstq     4.795159     7.157459\n",
      "36        avg23_thirdq     4.346922     6.476417\n",
      "37           var23_min     0.000000     0.078029\n",
      "38           var23_max     2.232512     2.747658\n",
      "39          var23_mean     1.055374     1.208580\n",
      "40        var23_median     0.988055     1.141648\n",
      "41           var23_std     0.475613     0.542019\n",
      "42        var23_firstq     0.685139     0.803059\n",
      "43        var23_thirdq     1.391231     1.593066\n"
     ]
    }
   ],
   "source": [
    "#[5]\n",
    "time_domain_col_names = time_domain_df.columns.values.tolist()\n",
    "\n",
    "time_domain_intervals = []\n",
    "\n",
    "for this_col_name in time_domain_col_names:\n",
    "    this_col_df = time_domain_df[this_col_name]\n",
    "\n",
    "    this_col_array = np.array(this_col_df)\n",
    "    #[6] [7]\n",
    "    this_col_array = (this_col_array, )\n",
    "    #[6] [7]\n",
    "    this_col_results = bootstrap(this_col_array, np.std, confidence_level=0.9, method=\"percentile\")\n",
    "\n",
    "    time_domain_intervals.append([this_col_name, this_col_results.confidence_interval.low, this_col_results.confidence_interval.high])\n",
    "\n",
    "time_domain_int_df = pd.DataFrame(time_domain_intervals, columns=[\"time_series_feature\", \"lower_bound\", \"upper_bound\"])\n",
    "\n",
    "print(time_domain_int_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    time_series_feature  lower_bound  upper_bound\n",
    "# 0             instance    23.221631    27.189456\n",
    "# 1        test_or_train     0.354829     0.456247\n",
    "# 2            avg12_min     8.234602    10.711499\n",
    "# 3            avg12_max     3.325309     5.258153\n",
    "# 4           avg12_mean     4.679485     5.855100\n",
    "# 5         avg12_median     4.768167     5.961253\n",
    "# 6            avg12_std     1.561170     1.939305\n",
    "# 7         avg12_firstq     5.545619     6.614971\n",
    "# 8         avg12_thirdq     4.307069     5.812099\n",
    "# 9            var12_min     0.000000     0.000000\n",
    "# 10           var12_max     4.598930     5.383356\n",
    "# 11          var12_mean     1.383190     1.694530\n",
    "# 12        var12_median     1.231027     1.535567\n",
    "# 13           var12_std     0.798162     0.936493\n",
    "# 14        var12_firstq     0.826688     1.029479\n",
    "# 15        var12_thirdq     1.884852     2.283083\n",
    "# 16           avg13_min     2.747719     3.094121\n",
    "# 17           avg13_max     4.147518     5.437769\n",
    "# 18          avg13_mean     3.403756     4.468950\n",
    "# 19        avg13_median     3.412787     4.514471\n",
    "# 20           avg13_std     0.758398     1.113562\n",
    "# 21        avg13_firstq     3.597277     4.678926\n",
    "# 22        avg13_thirdq     3.516211     4.651840\n",
    "# 23           var13_min     0.000000     0.000000\n",
    "# 24           var13_max     1.964227     2.347642\n",
    "# 25          var13_mean     1.066697     1.216503\n",
    "# 26        var13_median     1.048360     1.194443\n",
    "# 27           var13_std     0.419664     0.483065\n",
    "# 28        var13_firstq     0.769230     0.885296\n",
    "# 29        var13_thirdq     1.425630     1.618564\n",
    "# 30           avg23_min     4.393033     7.487510\n",
    "# 31           avg23_max     4.722807     6.523554\n",
    "# 32          avg23_mean     4.391391     6.690322\n",
    "# 33        avg23_median     4.489899     6.862770\n",
    "# 34           avg23_std     0.811236     1.213073\n",
    "# 35        avg23_firstq     4.792746     7.173819\n",
    "# 36        avg23_thirdq     4.356091     6.486624\n",
    "# 37           var23_min     0.000000     0.078029\n",
    "# 38           var23_max     2.239113     2.739993\n",
    "# 39          var23_mean     1.057849     1.209059\n",
    "# 40        var23_median     0.988929     1.141793\n",
    "# 41           var23_std     0.476923     0.542358\n",
    "# 42        var23_firstq     0.685978     0.802921\n",
    "# 43        var23_thirdq     1.395187     1.591946"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Select Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In selecting features, I'm going to try to look at the features that have the largest standard deviation. A feature with a large standard deviation means a high variance in values. This would imply that these values change the most across all the data.\n",
    "\n",
    "To find this, I will calculate the average of the standard deviations for each measure (min, max, mean, etc.) across the six time series.\n",
    "\n",
    "(Assistance from TA Zizhao Hu, who recommended that I take a look at the measures with the largest variance.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_average 3.1160460000000003\n",
      "max_average 4.129335333333333\n",
      "mean_average 3.1524634999999996\n",
      "median_average 3.155756\n",
      "std_average 0.9339541666666668\n",
      "firstq_average 3.1698838333333335\n",
      "thirdq_average 3.3406070000000003\n"
     ]
    }
   ],
   "source": [
    "min_average = (9.569975 + 0.000000 + 2.956462 + 0.000000 + 6.124001 + 0.045838)/6.0\n",
    "\n",
    "max_average = (4.394362 + 5.062729 + 4.875137 + 2.183625 + 5.741238 + 2.518921)/6.0\n",
    "\n",
    "mean_average = (5.335718 + 1.574164 + 4.008380 + 1.166114 + 5.675593 + 1.154812)/6.0\n",
    "\n",
    "median_average = (5.440054 + 1.412244 + 4.036396 + 1.145586 + 5.813782 + 1.086474)/6.0\n",
    "\n",
    "std_average = (1.772153 + 0.884105 + 0.946710 + 0.458242 + 1.024898 + 0.517617)/6.0\n",
    "\n",
    "firstq_average = (6.153590 + 0.946386 + 4.220658 + 0.843620 + 6.096465 + 0.758584)/6.0\n",
    "\n",
    "thirdq_average = (5.138925 + 2.125266 + 4.171628 + 1.552504 + 5.531720 + 1.523599)/6.0\n",
    "\n",
    "print(\"min_average\", min_average)\n",
    "print(\"max_average\", max_average)\n",
    "print(\"mean_average\", mean_average)\n",
    "print(\"median_average\", median_average)\n",
    "print(\"std_average\", std_average)\n",
    "print(\"firstq_average\", firstq_average)\n",
    "print(\"thirdq_average\", thirdq_average)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "min_average 3.1160460000000003\n",
    "\n",
    "max_average 4.129335333333333\n",
    "\n",
    "mean_average 3.1524634999999996\n",
    "\n",
    "median_average 3.155756\n",
    "\n",
    "std_average 0.9339541666666668\n",
    "\n",
    "firstq_average 3.1698838333333335\n",
    "\n",
    "thirdq_average 3.3406070000000003\n",
    "\n",
    "So it appears that the measures that have the highest standard deviations across the six time series are \"max\", \"third quartile\", and \"first quartile\". Thus, I will be using those measures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ISLR 3.7.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Linear Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say that there is not enough information.\n",
    "\n",
    "A cubic function can be seen as linear at a small enough range. (For instance, y = x^3 looks very linear very close to x=0.) Thus, even if the underlying true relationship is linear, the cubic function can be fitted and scaled such that it is very close to linear around the range of the training data. Due to noise, this could cause the RSS for the cubic regression model to actually be lower than the RSS of the linear regression model. (It could just so happen that the cubic regression model fits this certain training dataset better.)\n",
    "\n",
    "In addition, a cubic regression model could also overfit the data. This will reduce the cubic model's RSS for the training data, and could make it less than the linear model's RSS.\n",
    "\n",
    "All of this doesn't mean that the linear regression model will always have a greater RSS than the cubic model -- it could have a smaller RSS than the cubic model. However, given that we don't know how much noise is in the data, I would say that there isn't enough information to make a strong conclusion either way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Linear Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say that the linear regression model will have a lower RSS.\n",
    "\n",
    "As the true underlying relationship is linear, it's much more likely that the new test data will generally fit more closely the linear regression model, as opposed to the cubic regression model. This is because outside of the range of the training data, the cubic regression model could greatly diverge from the underlying linear relationship. (y = x^3 is close to linear for a small section near x=0, but farther away from that, it widely diverges from being linear.) In addition, the cubic regression model could be overfitted, meaning that it will not be able to generalize that well to new data as compared to the linear regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Not Linear Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say we do not have enough information.\n",
    "\n",
    "Like part a, we don't know how much noise is in the data. Furthermore, a cubic function can be fitted and scaled to be linear-like (or even quadratic-like) around the range of the test data, possibly resulting in a lower RSS for the cubic function as compared to the linear function. (For instance, if the underlying relationship is quadratic, the cubic function can be scaled/fitted to be quadratic-like around the range of the training data. This would then perform better than the linear regression model.)\n",
    "\n",
    "Most importantly, however, is the fact that we do not know how far away from linear our true underlying relationship is. If our true underlying relationship is very close to linear, then the linear regression model will tend to have a lower RSS value. However, if the true underlying relationship is much farther away from linear, then the linear regression model will tend to have a higher RSS value.\n",
    "\n",
    "Given all these uncertainties, I would say we do not have enough information to make a reasonable expectation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Not Linear Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say we do not have enough information.\n",
    "\n",
    "As mentioned in part c, we do not know how close or far away the true underlying relationship is from being linear. If the true relationship is close to linear, then it's more likely that the linear regression model will fit the new test data well, and less likely that the cubic regression model will fit the test data well, resulting in a lower RSS for the linear regression model as compared to the cubic regression model. On the other hand, the true relationship could be closer to cubic, meaning that the cubic model could fit the true relationship much better than the linear model, meaning that the cubic model will tend to have a much lower RSS as compared to the cubic model.\n",
    "\n",
    "In other words, given that we don't know the underlying true relationship, it's difficult to judge if a linear or a cubic model will work better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.3 - Extra Practice "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.5 - Extra Practice "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] https://www.geeksforgeeks.org/how-to-iterate-over-files-in-directory-using-python/#\n",
    "\n",
    "[2] https://www.w3schools.com/python/pandas/pandas_csv.asp\n",
    "\n",
    "[3] https://towardsdatascience.com/a-brief-introduction-to-time-series-classification-algorithms-7b4284d31b97\n",
    "\n",
    "[4] https://machinelearningmastery.com/machine-learning-data-transforms-for-time-series-forecasting/\n",
    "\n",
    "[5] https://sparkbyexamples.com/pandas/pandas-get-column-names/\n",
    "\n",
    "[6] https://www.statology.org/bootstrapping-in-python/\n",
    "\n",
    "[7] https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
